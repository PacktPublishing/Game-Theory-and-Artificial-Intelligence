{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37dbe8d1",
   "metadata": {},
   "source": [
    "# Q-Learning in GridWorld\n",
    "\n",
    "This notebook demonstrates how Q-learning, a type of reinforcement learning algorithm, can be used to train an agent to navigate a grid-based environment called GridWorld. The agent's objective is to find an optimal path from the start state to the goal state while avoiding obstacles. The agent learns by interacting with the environment and updating its knowledge base, represented by a Q-table, based on the rewards received for its actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010249fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb318677",
   "metadata": {},
   "source": [
    "## Define the GridWorld Environment with Random Obstacles\n",
    "In this cell, we define the `GridWorld` class that simulates the environment. The grid has a start state at the top-left corner, a goal state at the bottom-right corner, and random obstacles. The agent can move up, down, left, or right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76f6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, grid_size=(50, 50), num_obstacles=100):\n",
    "        self.grid_size = grid_size\n",
    "        self.num_obstacles = num_obstacles\n",
    "        self.state = (0, 0)  # Start state\n",
    "        self.goal_state = (grid_size[0]-1, grid_size[1]-1)  # Goal state at the bottom-right corner\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.rewards = np.zeros(grid_size)\n",
    "        self.rewards[self.goal_state] = 1  # Reward for reaching the goal\n",
    "        self.obstacle_states = self._place_obstacles()\n",
    "        for obstacle in self.obstacle_states:\n",
    "            self.rewards[obstacle] = -1  # Penalty for hitting the obstacle\n",
    "\n",
    "    def _place_obstacles(self):\n",
    "        obstacles = set()\n",
    "        while len(obstacles) < self.num_obstacles:\n",
    "            obstacle = (random.randint(0, self.grid_size[0]-1), random.randint(0, self.grid_size[1]-1))\n",
    "            if obstacle != self.state and obstacle != self.goal_state:\n",
    "                obstacles.add(obstacle)\n",
    "        return list(obstacles)\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = list(self.state)\n",
    "        if action == 0:  # Up\n",
    "            next_state[0] = max(0, self.state[0] - 1)\n",
    "        elif action == 1:  # Down\n",
    "            next_state[0] = min(self.grid_size[0] - 1, self.state[0] + 1)\n",
    "        elif action == 2:  # Left\n",
    "            next_state[1] = max(0, self.state[1] - 1)\n",
    "        elif action == 3:  # Right\n",
    "            next_state[1] = min(self.grid_size[1] - 1, self.state[1] + 1)\n",
    "        \n",
    "        self.state = tuple(next_state)\n",
    "        reward = self.rewards[self.state]\n",
    "        done = self.state == self.goal_state\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def print_grid(self):\n",
    "        grid = np.full(self.grid_size, ' ')\n",
    "        grid[self.state] = 'S'\n",
    "        grid[self.goal_state] = 'G'\n",
    "        for obstacle in self.obstacle_states:\n",
    "            grid[obstacle] = 'X'\n",
    "        \n",
    "        print('\\n'.join(' '.join(row) for row in grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc8a788",
   "metadata": {},
   "source": [
    "## Introduction to Q-Learning\n",
    "\n",
    "Q-learning is a model-free reinforcement learning algorithm where an agent learns the value (Q-value) of taking certain actions in given states. The agent aims to learn an optimal policy that maximizes the expected sum of rewards over time. The Q-values are updated iteratively based on the rewards received for each action using the Bellman equation:\n",
    "\n",
    "\\[ Q(s, a) = Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right] \\]\n",
    "\n",
    "where:\n",
    "- \\( \\alpha \\) is the learning rate.\n",
    "- \\( \\gamma \\) is the discount factor.\n",
    "- \\( r \\) is the reward received after taking action \\( a \\) in state \\( s \\).\n",
    "- \\( s' \\) is the next state after taking action \\( a \\).\n",
    "- \\( \\max_{a'} Q(s', a') \\) is the maximum Q-value for all possible actions \\( a' \\) in the next state \\( s' \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ce11a",
   "metadata": {},
   "source": [
    "## Define the Q-Learning Algorithm\n",
    "\n",
    "In this cell, we implement the Q-learning algorithm. The agent will learn to navigate the grid by exploring different actions and updating the Q-values based on the rewards received. The algorithm uses an epsilon-greedy policy to balance exploration (random actions) and exploitation (choosing the best-known action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eef1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    q_table = np.zeros((*env.grid_size, len(env.actions)))\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.choice(range(len(env.actions)))  # Explore: random action\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])  # Exploit: best action based on Q-table\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            old_value = q_table[state][action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            \n",
    "            # Update Q-value using the Bellman equation\n",
    "            q_table[state][action] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "            \n",
    "            state = next_state\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4d55b",
   "metadata": {},
   "source": [
    "## Initialize the Environment and Run Q-Learning\n",
    "We initialize the environment with a 8x8 grid and 16 random obstacles. We then run the Q-learning algorithm to train the agent. The Q-table is printed to show the learned values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7f52cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Grid:\n",
      "S   X    \n",
      "    X   X\n",
      "         \n",
      "        X\n",
      "    X   G\n",
      "Q-Table:\n",
      "[[[ 0.37111501  0.36239623  0.39950067  0.4782969 ]\n",
      "  [ 0.43903665  0.531441    0.4134855  -0.61418559]\n",
      "  [-0.95788924 -0.9941375   0.4707156   0.        ]\n",
      "  [ 0.          0.         -0.83376539  0.        ]\n",
      "  [ 0.         -1.          0.          0.        ]]\n",
      "\n",
      " [[ 0.08178877  0.          0.05854693  0.53043491]\n",
      "  [ 0.45907214  0.59049     0.36816678 -0.45327411]\n",
      "  [-0.99995502  0.65209693  0.14392283  0.        ]\n",
      "  [ 0.          0.05896304 -0.97103386 -0.99999985]\n",
      "  [ 0.          0.          0.         -0.9991405 ]]\n",
      "\n",
      " [[ 0.43138201  0.          0.          0.        ]\n",
      "  [ 0.50440949  0.46078981  0.27653242  0.6561    ]\n",
      "  [-0.53223531  0.729       0.58460229  0.54901241]\n",
      "  [ 0.          0.1539      0.65572999  0.03583722]\n",
      "  [-0.40951    -0.40951     0.46412155  0.0391123 ]]\n",
      "\n",
      " [[ 0.07676482  0.          0.          0.        ]\n",
      "  [ 0.58843878  0.          0.00545716  0.13731516]\n",
      "  [ 0.63468436 -0.19048993  0.48067413  0.81      ]\n",
      "  [ 0.52427916  0.9         0.64583263 -0.91192645]\n",
      "  [ 0.22291985  0.          0.         -0.1       ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 0.09632226  0.          0.         -0.3439    ]\n",
      "  [ 0.13850757 -0.6763614   0.00539333  0.89999124]\n",
      "  [ 0.76166195  0.86481059 -0.18683269  1.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment with a 8x8 grid and 16 obstacles\n",
    "grid_size = (5, 5)\n",
    "num_obstacles = 5\n",
    "env = GridWorld(grid_size, num_obstacles)\n",
    "\n",
    "# Print the initial grid\n",
    "print(\"Initial Grid:\")\n",
    "env.print_grid()\n",
    "\n",
    "# Run Q-learning\n",
    "q_table = q_learning(env)\n",
    "\n",
    "# Print the Q-table\n",
    "print(\"Q-Table:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7731ee02",
   "metadata": {},
   "source": [
    "## Extract and Visualize the Policy\n",
    "\n",
    "After training, we extract the policy from the Q-table by selecting the action with the highest Q-value for each state. We visualize the policy using symbols to represent each action (up, down, left, right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf1ed76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy:\n",
      "→ ↓ ← ↑ ↑\n",
      "→ ↓ ↓ ↓ ↑\n",
      "↑ → ↓ ← ←\n",
      "↑ ↑ → ↓ ↑\n",
      "↑ ↑ → → ↑\n"
     ]
    }
   ],
   "source": [
    "# Extract policy\n",
    "policy = np.argmax(q_table, axis=2)\n",
    "policy_symbols = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
    "\n",
    "print(\"\\nPolicy:\")\n",
    "for row in policy:\n",
    "    print(' '.join(policy_symbols[action] for action in row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c357e3",
   "metadata": {},
   "source": [
    "## Demonstrate the Learned Policy\n",
    "\n",
    "We demonstrate the learned policy by showing the agent's path from the start state to the goal state. This helps visualize how the agent navigates the grid based on the learned Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c68d8328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path taken: [(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (3, 2), (3, 3), (4, 3), (4, 4)]\n",
      "Goal reached!\n",
      "\n",
      "Path taken: [(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (3, 2), (3, 3), (4, 3), (4, 4)]\n",
      "Goal reached!\n",
      "\n",
      "Path taken: [(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (3, 2), (3, 3), (4, 3), (4, 4)]\n",
      "Goal reached!\n",
      "\n",
      "Path taken: [(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (3, 2), (3, 3), (4, 3), (4, 4)]\n",
      "Goal reached!\n",
      "\n",
      "Path taken: [(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (3, 2), (3, 3), (4, 3), (4, 4)]\n",
      "Goal reached!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the learned policy\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    path = [state]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done = env.step(action)\n",
    "        path.append(next_state)\n",
    "        state = next_state\n",
    "    print(f\"Path taken: {path}\")\n",
    "    print(\"Goal reached!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
